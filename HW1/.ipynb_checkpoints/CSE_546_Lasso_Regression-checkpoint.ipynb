{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Fleming Oct 2015\n",
    "\n",
    "CSE 546 HW 1 Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "\n",
      "(3, 4) (4,)\n",
      "\n",
      "[  6.  22.  38.]\n",
      "\n",
      "[ 12.]\n",
      "\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This is a quick walkthrough to help you understand the operations in scipy.sparse\n",
    "####\n",
    "\n",
    "# construct a sparse array, here we simply construct it from dense array\n",
    "A = np.arange(12).reshape(3,4)\n",
    "print A\n",
    "X = sp.csc_matrix(A)\n",
    "\n",
    "w = np.ones(4)\n",
    "print\n",
    "print X.shape, w.shape\n",
    "\n",
    "#  matrix vector multiplication\n",
    "y = X.dot(w)\n",
    "print\n",
    "print y\n",
    "\n",
    "#\n",
    "# dot product between i-th column of X and g\n",
    "#\n",
    "i = 0\n",
    "g = np.ones(3)\n",
    "# r1 = dot(X[:,i], g), because X takes matrix syntax, we need to do it in this way\n",
    "r1 = X[:,i].T.dot(g)\n",
    "print\n",
    "print r1\n",
    "#\n",
    "# This is how you can get dot(X[:,i], X[:,i]) in csc_matix\n",
    "#\n",
    "r2 = X[:,i].T.dot(X[:,i])[0,0]\n",
    "print\n",
    "print r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_norm_data(n,k,d,sigma=1):\n",
    "    \"\"\"\n",
    "    Generates independent data pairs (x_i,y_i) according to the following model:\n",
    "    \n",
    "    yi = w*_0 + w*_1x_i_1 + w*2 x_i_2 + ... w*k x_i_k + eps_i\n",
    "    \n",
    "    for eps_i = Gaussian noise of the form N(0,sigma^2)\n",
    "    and each element of X (shape N x d) is from N(0,1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of samples\n",
    "    k : int\n",
    "        k < d number of features for dimensions d\n",
    "    d : int\n",
    "        number of dimensions\n",
    "    sigma : float\n",
    "        Gaussian error standard deviation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : vector\n",
    "        true weight vector\n",
    "    X : n x d matrix\n",
    "        data matrix\n",
    "    y : n x 1 vector\n",
    "    \"\"\"\n",
    "    assert(k < d), \"k < d must hold for k: %lf, d: %lf\" % (k,d)\n",
    "    \n",
    "    #Create w vector\n",
    "    #Let w0 = 0 and create a w∗ by setting the first k elements to ±10 \n",
    "    #(choose any sign pattern) and the remaining elements to 0\n",
    "    w = np.zeros(d)\n",
    "    for i in range(1,k+1):\n",
    "        if i < k/2:\n",
    "            w[i] = 10\n",
    "        else:\n",
    "            w[i] = -10\n",
    "    \n",
    "    #Generate n x d data matrix X for each element from N(0,1)\n",
    "    X = sp.csc_matrix(np.random.randn(n, d))\n",
    "    \n",
    "    #Generate n x 1 array of Gaussian error samples from N(0,sigma^2)\n",
    "    eps = np.random.randn(n)\n",
    "    \n",
    "    #Finally, generate a Gaussian noise vector eps with variance σ^2 and \n",
    "    #form y = Xw* + w*_0 + eps for w*_0 assumed to be 0\n",
    "    y = X.dot(w) + eps\n",
    "        \n",
    "    return w, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_solution(X,y,w_pred,w_0_pred,l):\n",
    "    \"\"\"\n",
    "    See if the computed solution w_pred, w_0_pred for a given lambda l\n",
    "    is correct.  That occurs when:\n",
    "    \n",
    "    test = 2X^T(Xw_pred + w_0_pred - y)\n",
    "    test_j = -lambda*sign(w_pred_j) for each j that is nonzero\n",
    "    Otherwise, each j value should be lesser in magnitude that lambda\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : n x d matrix of data\n",
    "    y : N x 1 vector of response variables\n",
    "    w_pred : predicted d dimensions weight vector\n",
    "    w_0_pred : scalar offset term\n",
    "    l : regularization tuning parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ans : bool\n",
    "        whether or not the solution passes the test criteria\n",
    "    \"\"\"\n",
    "    eps = 1.0e-8\n",
    "    condition = False\n",
    "    \n",
    "    test = 2.0*X.T*(X.dot(w_pred) + w_0_pred - y)\n",
    "    \n",
    "    #Mask values corresponding to w_pred == 0\n",
    "    mask = np.fabs(w_pred) < eps\n",
    "    mask2 = np.fabs(test[mask]) < l\n",
    "    mask = np.ones(len(w_pred))[mask]\n",
    "            \n",
    "    if np.array_equal(mask2,mask) and np.sum(mask) != len(w_pred):\n",
    "        w_j = test[np.logical_not(mask)]\n",
    "        if np.allclose(w_j,-l*np.sign(w_j),atol=1.0e-10, rtol=1.0e-1) and w_j != []:\n",
    "            condition = True\n",
    "        else:\n",
    "            condition = False\n",
    "    else:\n",
    "        condition = False\n",
    "    \n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_max_lambda(X,y):\n",
    "    \"\"\"\n",
    "    Compute the smallest lambda for which the solution w is entirely zero\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : n x d matrix of data (scipy sparse matrix)\n",
    "    y : n x 1 vector of response variables\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    l : float\n",
    "        Smallest value of lambda for which the solution w is entirely zero\n",
    "    \"\"\"\n",
    "    y_mean = np.mean(y)\n",
    "    arg = X.T.multiply(y - y_mean)\n",
    "    return 2.0*np.linalg.norm(arg,ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_lasso(X,y,l=10,w=-999,w_0=-999):\n",
    "    \"\"\"\n",
    "    Implimentation of the naive (un-optimized) lasso regression \n",
    "    algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : n x d matrix of data\n",
    "    X_i : the ith row of X\n",
    "    y : N x 1 vector of response variables\n",
    "    w : d dimensions weight vector (optional)\n",
    "    w_0 : scalar offset term (optional)\n",
    "    l : regularization tuning parameter\n",
    "    \n",
    "    All matrices X assumed to be sparse and of the form given by \n",
    "    scipy.sparse.csc matrix\n",
    "    \n",
    "    Algorithm 1: Coordinate Descent Algorithm for Lasso\n",
    "    \n",
    "    while not converged do:\n",
    "        w_0 <- sum_i=1_N[y_i - sum_j[w_j X_ij]]/N\n",
    "        for(k [1,d]) do:\n",
    "            a_k <- 2 * sum_i=1_N[X_ik ^2]\n",
    "            c_k <- 2 * sum_i=1_N[X_ik (y_i - (w_0 + sum_j!=k[w_j X_ij]))]\n",
    "            w_k <- (c_k + lambda)/a_k if c_k < -lambda\n",
    "                    0 if c_k is between [-lambda,lambda]\n",
    "                    (c_k - lambda)/a_k if c_k > lambda\n",
    "        end\n",
    "    end\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array\n",
    "        d x 1 weight vector\n",
    "    w_0 : float\n",
    "        offset\n",
    "        \n",
    "    \"\"\"\n",
    "    #Define values\n",
    "    N = y.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    #Convergence condition\n",
    "    eps = 1.0e-3\n",
    "    w_old = np.zeros(w.shape)\n",
    "    w_pred = np.copy(w)\n",
    "    \n",
    "    while((w_pred - w_old).dot(w_pred - w_old) > eps):\n",
    "        #Store for convergence test \n",
    "        w_old = np.copy(w_pred)\n",
    "        \n",
    "        #Compute w_0\n",
    "        w_0 = np.sum(y)\n",
    "        w_0 -= X.dot(w_pred).sum()\n",
    "        w_0 /= N\n",
    "            \n",
    "        #Compute a_k: d x 1 summing over columns\n",
    "        a = 2.0*np.asarray((X.power(2).sum(axis=0).T))\n",
    "        c = np.zeros(d)\n",
    "            \n",
    "        for k in range(0,d):\n",
    "            #Compute c_k: d x 1\n",
    "            c_sum = 0.0\n",
    "            for i in range(0,N):\n",
    "                #Select not k columns\n",
    "                ind = [x for x in range(0,d) if x != k]\n",
    "                c_sum += X[i,k]*(y[i] - (X[i,ind].dot(w[ind]) + w_0))\n",
    "            c[k] = 2.0*c_sum\n",
    "            \n",
    "            #Compute w_k\n",
    "            if(c[k] < -l):\n",
    "                w_pred[k] = (c[k] + l)/a[k]\n",
    "            elif(c[k] >= -l and c[k] <= l):\n",
    "                w_pred[k] = 0.0\n",
    "            elif(c[k]  > l):\n",
    "                w_pred[k] = (c[k] - l)/a[k]\n",
    "            else:\n",
    "                print \"Error! Shouldn't ever happen.\"\n",
    "        #end for\n",
    "        #print w_pred\n",
    "    #end while\n",
    "    \n",
    "    #Return as row array\n",
    "    return w_pred, w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso_reg_path(X,y,w=-999,w_0=-999,scale=0.8,reg_type=\"naive\"):\n",
    "    \"\"\"\n",
    "    Implimentation of the naive (un-optimized) lasso regression \n",
    "    algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : n x d matrix of data\n",
    "    y : n x 1 vector of response variables\n",
    "    w : d dimensions weight vector (optional)\n",
    "    w_0 : scalar offset term (optional)\n",
    "    scale : by how much lambda l decreases each run\n",
    "    reg_type : str\n",
    "        naive = use slow naive lasso\n",
    "        quick = use optimized lasso\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    All matrices assumed to be sparse and of the form given by \n",
    "    scipy.sparse.csc matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    #Choose upper bound for lambda, initial conditions\n",
    "    l = compute_max_lambda(X,y)*scale\n",
    "    \n",
    "    #If no initial conditions, assume Gaussian\n",
    "    if w == -999:\n",
    "        w = np.random.randn(d)\n",
    "    if w_0 == -999:\n",
    "        w_0 = np.random.randn(1)\n",
    "    \n",
    "    w_pred = np.copy(w)\n",
    "    w_0_pred = w_0\n",
    "        \n",
    "    #If solution isn't converged, keep going\n",
    "    while(l > 1):\n",
    "        w_pred, w_0_pred = naive_lasso(X,y,l=l,w=w_pred,w_0=w_0_pred)        \n",
    "        \n",
    "        #If solution is sufficiently good, return it\n",
    "        if check_solution(X,y,w_pred=w_pred,w_0_pred=w_0_pred,l=l):\n",
    "            return w_pred, w_0_pred, l\n",
    "        \n",
    "        #Decrease scale for next iteration\n",
    "        l *= scale\n",
    "    \n",
    "    return w_pred, w_0_pred, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0. -10. -10.   0.   0.]\n",
      "[  0.88355952  -9.96892892 -10.20455824   0.6623346   -0.49756506]\n",
      "0.966711373384\n",
      "\n",
      "0.98856065044\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "k = 2\n",
    "d = 5\n",
    "l = 2750\n",
    "w, X, y = generate_norm_data(N,k,d)\n",
    "\n",
    "#print(w.shape)\n",
    "#print\n",
    "#print(X.shape)\n",
    "#print\n",
    "#print(y.shape)\n",
    "\n",
    "#w_pred, w_0_pred = naive_lasso(X,y,l=l)\n",
    "w_test = -999\n",
    "w_0_test = -999\n",
    "w_pred, w_0_pred, l = lasso_reg_path(X,y,w=w_test,w_0=w_0_test,scale=0.75,reg_type=\"naive\")\n",
    "\n",
    "#print w\n",
    "print w\n",
    "print w_pred\n",
    "print l\n",
    "print\n",
    "\n",
    "SSres = np.sum(np.power(w_pred-w,2))\n",
    "w_bar = np.mean(w)\n",
    "SStot = np.sum(np.power(w_pred-w_bar,2))\n",
    "print(1.0 - (SSres/SStot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print check_solution(X,y,w_pred,w_0_pred,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2750.0556728012771"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_max_lambda(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
